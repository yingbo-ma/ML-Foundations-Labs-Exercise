{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e41ecc-91f8-4921-8ca6-28479f17f4c0",
   "metadata": {},
   "source": [
    "### **Word2Vec, Naive Bayes, and K-Nearest Neighbor**\n",
    "\n",
    "#### **Introduction**\n",
    "In this assignment, we will explore three fundamental concepts: Word2Vec, Naïve Bayes, and K-Nearest Neighbors (KNN). \n",
    "\n",
    "- **Word2Vec**: A Vector Space Model for converting words to vectors while capturing semantic relationships between words. It converts text into dense vector representations, making it useful for various NLP tasks like similarity detection and sentiment analysis.\n",
    "- **Naive Bayes**: A probabilistic machine learning algorithm based on Bayes' theorem that is widely used in text classification tasks, such as spam detection and sentiment analysis. It assumes conditional independence among features, making it computationally efficient.\n",
    "- **K-Nearest Neighbors (KNN)**: A deterministic machine learning algorithm that classifies data points based on the majority vote of their k-nearest neighbors. It is commonly applied in pattern recognition and recommendation systems.\n",
    "\n",
    "#### **Instructions**\n",
    "There are three tasks in this lab exercise:\n",
    "- Task 1: Word2Vec\n",
    "- Task 2: Naive Bayes\n",
    "- Task 3: K-Nearest Neighbors (KNN)\n",
    "\n",
    "In each task, you will see an **example code** first. This example code is to help you understand the basic python implementation in order to complete each task.\n",
    "\n",
    "You will then see a **practice task**. This practice task is the real assignment you need to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a670a2-0031-4649-b74e-d60d8eff8c4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Task 1: Word2Vec** (Example Code)\n",
    "This is an example code for you to understand the basic python implementation of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45f400-dbc8-40cd-b585-cd6e49014007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python dependencies\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b375c-515d-42fb-892d-77b76cdf2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "import nltk  # Natural Language Toolkit for text processing\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer for splitting text into words\n",
    "from gensim.models import Word2Vec  # Word2Vec model for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72edfdc4-2862-4dd8-9c6b-8af73b592d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666119b-ed66-44d9-b59a-f5726e7abfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 1: Build your text training corpus. This corpus is used to train your Word2Vec model.\n",
    "\"\"\"\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Deep learning and neural networks are powerful\",\n",
    "    \"Natural language processing is a subset of AI\",\n",
    "    \"Word embeddings capture word meanings\",\n",
    "    \"Naïve Bayes is a probabilistic classifier\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c3a1e-03d8-4950-a86a-5c0e955dc4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 2: Tokenize the text corpus. This is a necessary step for any natural language processing tasks. \n",
    "           ML models learn by words, not sentences.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizing the corpus (splitting sentences into words)\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c7188-a5b0-4fa1-bfd1-45f01e9f18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 3: Train Word2Vec model with the tokenized corpus.\n",
    "\"\"\"\n",
    "\n",
    "# Train a Word2Vec model to generate word embeddings from the tokenized corpus.\n",
    "# Word2Vec is a neural network-based algorithm that learns vector representations of words.\n",
    "# These word embeddings capture semantic relationships between words.\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,  # The input corpus, where each sentence is tokenized into a list of words.\n",
    "    vector_size=50,              # The dimensionality of the word vectors. Higher values capture more semantic information but require more computation.\n",
    "    window=5,                    # The maximum distance between the target word and neighboring words in a sentence. A larger window captures more context.\n",
    "    min_count=1,                 # Ignores words that appear less than the specified count. Setting it to 1 ensures all words are included.\n",
    "    workers=4                    # The number of CPU threads to use for training. More workers speed up training but require more resources.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa95cca-f626-4473-8564-e37970535bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Step 4: Print the vector representation of a word\n",
    "## Reminder! You can NOT generate vectors for the words that are not in your training corpus.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Vector for the word 'learning':\", word2vec_model.wv['learning'], \"\\n\")\n",
    "print(\"Vector for the word 'learning':\", word2vec_model.wv['neural'], \"\\n\")\n",
    "print(\"Vector for the word 'learning':\", word2vec_model.wv['language'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c924747-dca1-4430-a30e-715141c27644",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Task 1: Word2Vec** (Practice Task)\n",
    "This is the assignment you need to complete. You may use any code snippets shown in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e6fa7-05c0-4bbd-8e65-b5f8fbaf0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Curate your own training corpus. Please add another 9 sentences to create a text corpus with a total of 10 sentences.\n",
    "Corpus = [\n",
    "    'The cat is sleeping on the mat.'\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464259de-a1df-4d73-a90a-4465543472e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize your text corpus\n",
    "tokenized_corpus = ...\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687428f-0d04-44e1-9c7a-81943fa745ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train your Word2Vec model, with an word embedding dimension of 5.\n",
    "word2vec_model = Word2Vec(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4fbd7-d5bf-41fe-9358-1ee1617cbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate and print out three word embeddings from your training corpus.\n",
    "first_word_embedding = ...\n",
    "second_word_embedding = ...\n",
    "third_word_embedding = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127be0b-1242-4b42-8c61-69a0184b6f09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Task 2: Naive Bayes** (Example Code)\n",
    "This is an example code for you to understand the basic python implementation of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efe3a4-385c-487f-a0da-fc289d5a3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python dependencies\n",
    "!pip install scikit-learn # Install sklearn library.\n",
    "\n",
    "## Scikit-learn (sklearn) is a Python library providing tools for machine learning and data analysis. \n",
    "## Sklearn offers a wide range of functionalities, including: \n",
    "## - Regression: Algorithms for predicting continuous values, such as linear regression.\n",
    "## - Classification: Algorithms for predicting categories, such as k-nearest neighbors.\n",
    "## - Clustering: Algorithms for grouping similar data points, such as k-means.\n",
    "## - Dimensionality reduction: Techniques for reducing the number of variables in a dataset.\n",
    "## - Model selection: Tools for comparing and tuning different machine learning models.\n",
    "## - Preprocessing: Methods for preparing data for machine learning, such as feature extraction, scaling and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c420b-dbda-4178-b470-be82a341b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import pandas for handling datasets\n",
    "from sklearn.model_selection import train_test_split  # Import function to split dataset into train and test sets\n",
    "from sklearn.naive_bayes import GaussianNB  # Import Gaussian Naïve Bayes model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Import metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19ce57-b418-4af2-930c-cb597ce8d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Let's create a synthetic dataset\n",
    "\n",
    "# Columns: Fever, Cough, Fatigue, Age, Has Disease (1 = Yes, 0 = No)\n",
    "data = np.array([\n",
    "    [1, 1, 0, 25, 1],  # Example: Patient with fever and cough, no fatigue, age 25, has disease\n",
    "    [0, 1, 1, 40, 1],  # Example: Patient with cough and fatigue, no fever, age 40, has disease\n",
    "    [1, 0, 1, 35, 0],  # Example: Patient with fever and fatigue, no cough, age 35, no disease\n",
    "    [0, 0, 0, 50, 0],  # Example: Patient with no symptoms, age 50, no disease\n",
    "    [1, 1, 1, 30, 1],  # Example: Patient with all symptoms, age 30, has disease\n",
    "    [0, 0, 1, 45, 0],  # Example: Patient with fatigue only, age 45, no disease\n",
    "    [1, 1, 0, 60, 1],  # Example: Patient with fever and cough, no fatigue, age 60, has disease\n",
    "    [1, 0, 1, 55, 0],  # Example: Patient with fever and fatigue, no cough, age 55, no disease\n",
    "    [0, 1, 1, 65, 1],  # Example: Patient with cough and fatigue, no fever, age 65, has disease\n",
    "    [1, 0, 0, 28, 0]   # Example: Patient with fever only, age 28, no disease\n",
    "])\n",
    "\n",
    "# In our dataset, the training data include each patient's four information: fever, cough, fatigue, and age.\n",
    "# In our dataset, the label is Has Disease (1 = Yes, 0 = No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6a669-c945-49d1-a75d-b7e48e05dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert the dataset to a pandas DataFrame\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Fever', 'Cough', 'Fatigue', 'Age', 'Has_Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d36210-5be1-49e7-977f-e96209b0924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split data into training and test sets\n",
    "\n",
    "X = df[['Fever', 'Cough', 'Fatigue', 'Age']]  # Features: Symptoms and age\n",
    "y = df['Has_Disease']  # Target variable: Disease presence (1 or 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Split 70% training, 30% testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad7b76-ff4c-41ca-bad6-66a921ab1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the Gaussian Naïve Bayes model\n",
    "\n",
    "model = GaussianNB()  # Create a GaussianNB classifier\n",
    "model.fit(X_train, y_train)  # Train the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb5e7c-ab79-4114-ba67-d394e6e83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Make predictions\n",
    "y_pred = model.predict(X_test)  # Predict disease presence on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067ecc1-2e1c-4615-8b17-5e30125548cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Print accuracy of the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))  # Print confusion matrix\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))  # Print precision, recall, F1-score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cf2f2-8280-4e1a-b1b4-e9ea2812f3df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Task 2: Naive Bayes** (Practice Task)\n",
    "This is the assignment you need to complete. You may use any code snippets shown in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcb0c4-3e75-4fad-9135-e8894b3514b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Expanded Dataset. \n",
    "# In the expanded dataset, we added another two features, Heart Rate, and Blood Pressure, with total patient number of 30\n",
    "\n",
    "expanded_data = np.array([\n",
    "    [1, 1, 0, 25, 80, 120, 1],\n",
    "    [0, 1, 1, 40, 85, 130, 1],\n",
    "    [1, 0, 1, 35, 78, 125, 0],\n",
    "    [0, 0, 0, 50, 72, 115, 0],\n",
    "    [1, 1, 1, 30, 90, 140, 1],\n",
    "    [0, 0, 1, 45, 76, 118, 0],\n",
    "    [1, 1, 0, 60, 88, 135, 1],\n",
    "    [1, 0, 1, 55, 82, 128, 0],\n",
    "    [0, 1, 1, 65, 74, 110, 1],\n",
    "    [1, 0, 0, 28, 79, 122, 0],\n",
    "    [0, 1, 0, 33, 77, 120, 0],\n",
    "    [1, 0, 1, 48, 86, 132, 1],\n",
    "    [0, 1, 1, 52, 81, 126, 1],\n",
    "    [1, 1, 1, 38, 92, 145, 1],\n",
    "    [0, 0, 0, 70, 70, 108, 0],\n",
    "    [1, 1, 0, 58, 84, 129, 1],\n",
    "    [0, 0, 1, 43, 75, 115, 0],\n",
    "    [1, 0, 0, 29, 78, 120, 0],\n",
    "    [0, 1, 1, 60, 83, 125, 1],\n",
    "    [1, 0, 1, 47, 87, 130, 1],\n",
    "    [1, 1, 1, 32, 89, 138, 1],\n",
    "    [0, 0, 0, 55, 73, 112, 0],\n",
    "    [1, 1, 0, 61, 85, 133, 1],\n",
    "    [0, 0, 1, 42, 76, 118, 0],\n",
    "    [1, 0, 0, 27, 79, 124, 0],\n",
    "    [0, 1, 1, 59, 80, 127, 1],\n",
    "    [1, 0, 1, 49, 88, 136, 1],\n",
    "    [1, 1, 0, 34, 91, 142, 1],\n",
    "    [0, 0, 0, 63, 74, 109, 0],\n",
    "    [1, 1, 1, 37, 93, 148, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b2153-8358-4484-be6c-5dd2805b489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert data to a dataframe with seven coloumns. \n",
    "# Columns names are 'Fever', 'Cough', 'Fatigue', 'Age', 'Heart_Rate', 'Blood_Pressure', and 'Has_Disease'\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b583fa-9f6e-458e-b1de-1f29f390b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split data into training and test sets. Split 70% training, 30% testing\n",
    "\n",
    "# Define feature columns and target variable\n",
    "X = df.drop(columns=['Has_Disease'])  # Features. Keep all columns except for column name = 'Has_Disease'\n",
    "y = df['Has_Disease']  # Target variable: Disease presence\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)  # Split 70% training, 30% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735751e-1cc4-4d6d-8f89-6c6d08a4afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Gaussian Naïve Bayes model\n",
    "\n",
    "model = ...  # Create a GaussianNB classifier\n",
    "model.fit(...)  # Train the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b3b14-e93a-4f27-8326-f65aa93df853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions\n",
    "\n",
    "y_pred = model.predict(...)  # Predict disease presence on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a066a5-0994-4006-8b61-c01fff4d7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model\n",
    "\n",
    "print(\"Accuracy:\", ...)  # Print accuracy of the model\n",
    "print(\"Confusion Matrix:\\n\", ...)  # Print confusion matrix\n",
    "print(\"Classification Report:\\n\", ...)  # Print precision, recall, F1-score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eaab75-dc0e-4429-a400-6ab4140f1e68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Task 3: K-Nearest Neighbors (KNN)** (Example Code)\n",
    "This is an example code for you to understand the basic python implementation of Naive Bayes.\n",
    "\n",
    "In this example, we will load a real dataset and visualize the data samples. So we need to install one python library: Seaborn. Seaborn is a Python data visualization library built on top of Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79508bc4-0f81-4e21-93dd-e022fc76a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Seaborn python library\n",
    "\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bf0bd-e96e-4086-9d2a-60ac09b1b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import required Python libraries\n",
    "\n",
    "import numpy as np  # Import NumPy for numerical computations\n",
    "import pandas as pd  # Import Pandas for data manipulation\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for data visualization\n",
    "import seaborn as sns  # Import Seaborn for statistical data visualization\n",
    "from sklearn.model_selection import train_test_split  # Import function to split dataset into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler for feature normalization\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Import KNeighborsClassifier to implement KNN algorithm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Import evaluation metrics\n",
    "from sklearn.datasets import load_iris  # Import Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafa6c5-4708-46ff-8671-331a35febcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the dataset\n",
    "iris = load_iris()  # Load the Iris dataset from sklearn\n",
    "# Convert the dataset into a Pandas DataFrame for easier manipulation\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "# Add the target labels to the DataFrame\n",
    "data['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc601da8-de5c-4d1d-a272-bee2145ae6fc",
   "metadata": {},
   "source": [
    "The Iris dataset is a well-known dataset in machine learning and statistics, originally introduced by Ronald Fisher in 1936. It is widely used for classification problems.\n",
    "\n",
    "Size of the Iris Dataset:\n",
    "- Total samples: 150\n",
    "- Features per sample: 4\n",
    "- Target classes: 3 (Setosa, Versicolor, Virginica)\n",
    "- Features (Independent Variables)\n",
    "\n",
    "Each sample in the dataset represents a flower and has four numerical features:\n",
    "- Sepal length (cm)\n",
    "- Sepal width (cm)\n",
    "- Petal length (cm)\n",
    "- Petal width (cm)\n",
    "- Labels (Dependent Variable)\n",
    "\n",
    "The target variable represents the species of the iris flower, which can be one of three classes:\n",
    "- 0 → Setosa\n",
    "- 1 → Versicolor\n",
    "- 2 → Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3e234-9ea5-422a-977b-d07491f46e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Let's display the first 5 rows of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cda3ea-0b9f-4841-917f-030bfa95c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Splitting data into training and testing sets\n",
    "\n",
    "\n",
    "X = data.drop(columns=['target'])  # Extract feature variables (independent variables)\n",
    "y = data['target']  # Extract target variable (dependent variable)\n",
    "# Split the data into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74101fa9-234a-4428-99f5-faf7ac28bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Standardizing features to normalize data distribution\n",
    "# Feature scaling essentially normalizes the range of values within each feature, allowing for a fair comparison between different features.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()  # Initialize the StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test data using the same scaler\n",
    "\n",
    "## You can try to print both X_train and X_train_scaled to see the effect of the feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac511e-4c6a-465d-9b24-226c19144375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Implementing K-Nearest Neighbors (KNN) algorithm\n",
    "\n",
    "k = 3  # Define the number of neighbors to consider (K-value)\n",
    "knn = KNeighborsClassifier(n_neighbors=k)  # Initialize KNN classifier with chosen K\n",
    "knn.fit(X_train_scaled, y_train)  # Train the classifier on the scaled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a328d6-a29e-471e-84db-eb0d0d10bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Make predictions on the test data\n",
    "\n",
    "\n",
    "y_pred = knn.predict(X_test_scaled)  # Predict the target labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274c6ee-b9ad-4dfa-8ef5-17c40be47ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate Model Performance\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # Print accuracy of model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))  # Print detailed classification metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))  # Print confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd991e8e-f7e4-40cf-a26d-4b745c39a7f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Task 3: K-Nearest Neighbors (KNN)** (Practice Task)\n",
    "This is the assignment you need to complete. You may use any code snippets shown in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017da78d-16f3-4605-af19-5b6f1b0f52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Let's load another real-world dataset: Breast Cancer Dataset\n",
    "from sklearn.datasets import load_breast_cancer  # Import Breast Cancer dataset\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer = load_breast_cancer()  # Load the dataset from sklearn\n",
    "# Convert the dataset into a Pandas DataFrame for easier manipulation\n",
    "data = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "# Add the target labels to the DataFrame\n",
    "data['target'] = cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260ef71-ff9a-4039-815f-1392e960867e",
   "metadata": {},
   "source": [
    "The Breast Cancer Wisconsin (Diagnostic) dataset is a renowned collection of data used extensively in machine learning and medical research.\n",
    "\n",
    "The dataset comprises 30 features, including mean, standard error, and \"worst\" or largest values, computed for each image. These features encapsulate various aspects of cell nuclei characteristics such as:\n",
    "- mean radius: Mean of distances from center to points on the perimeter.\n",
    "- mean texture: Standard deviation of gray-scale values.\n",
    "- mean perimeter: Perimeter of the tumor.\n",
    "- mean area: Area of the tumor.\n",
    "\n",
    "Out of the 569 patients in the dataset, the binary label distribution is: Benign: 357 (63%) and Malignant: 212 (37%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fcf8a-e285-451a-bbef-54dcddf345dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Let's display the first 5 rows of the dataset. Remeber the last column 'target' is the label. All other coloumns are features.\n",
    "\n",
    "data.head(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5a3d0-6ba9-488c-8dcb-9dd6f577ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Splitting data into training and testing sets\n",
    "\n",
    "X = data.drop(columns=['target'])  # Extract feature variables (independent variables)\n",
    "y = data['target']  # Extract target variable (dependent variable)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48354a-dbf3-4676-886b-163dab404eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Standardizing features to normalize data distribution\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbcbc0-29be-4da9-9633-47da6145eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Implementing K-Nearest Neighbors (KNN) algorithm\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeaacc-5d9e-4472-ab96-eeed0d8f382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Make predictions on the test data\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b9b63-50c0-4fb6-a86d-4c48b2c96295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate Model Performance\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e368d-3a93-4a80-86bf-3dfc7a7bd661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: change the k value in Step 5 and observe the difference outputs from the KNN model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
